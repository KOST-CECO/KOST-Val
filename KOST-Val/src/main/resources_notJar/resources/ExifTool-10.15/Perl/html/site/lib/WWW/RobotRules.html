<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>WWW::RobotRules - database of robots.txt-derived permissions</title>
<link rel="stylesheet" href="../../../Active.css" type="text/css" />
<link rev="made" href="mailto:" />
</head>

<body>
<table border="0" width="100%" cellspacing="0" cellpadding="3">
<tr><td class="block" valign="middle">
<big><strong><span class="block">&nbsp;WWW::RobotRules - database of robots.txt-derived permissions</span></strong></big>
</td></tr>
</table>

<p><a name="__index__"></a></p>
<!-- INDEX BEGIN -->

<ul>

	<li><a href="#name">NAME</a></li>
	<li><a href="#synopsis">SYNOPSIS</a></li>
	<li><a href="#description">DESCRIPTION</a></li>
	<li><a href="#robots_txt">ROBOTS.TXT</a></li>
	<li><a href="#robots_txt_examples">ROBOTS.TXT EXAMPLES</a></li>
	<li><a href="#see_also">SEE ALSO</a></li>
</ul>
<!-- INDEX END -->

<hr />
<p>
</p>
<h1><a name="name">NAME</a></h1>
<p>WWW::RobotRules - database of robots.txt-derived permissions</p>
<p>
</p>
<hr />
<h1><a name="synopsis">SYNOPSIS</a></h1>
<pre>
 use WWW::RobotRules;
 my $rules = WWW::RobotRules-&gt;new('MOMspider/1.0');</pre>
<pre>
 use LWP::Simple qw(get);</pre>
<pre>
 {
   my $url = &quot;<a href="http://some.place/robots.txt&quot">http://some.place/robots.txt&quot</a>;;
   my $robots_txt = get $url;
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
 }</pre>
<pre>
 {
   my $url = &quot;<a href="http://some.other.place/robots.txt&quot">http://some.other.place/robots.txt&quot</a>;;
   my $robots_txt = get $url;
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
 }</pre>
<pre>
 # Now we can check if a URL is valid for those servers
 # whose &quot;robots.txt&quot; files we've gotten and parsed:
 if($rules-&gt;allowed($url)) {
     $c = get $url;
     ...
 }</pre>
<p>
</p>
<hr />
<h1><a name="description">DESCRIPTION</a></h1>
<p>This module parses <em>/robots.txt</em> files as specified in
``A Standard for Robot Exclusion'', at
&lt;http://www.robotstxt.org/wc/norobots.html&gt;
Webmasters can use the <em>/robots.txt</em> file to forbid conforming
robots from accessing parts of their web site.</p>
<p>The parsed files are kept in a WWW::RobotRules object, and this object
provides methods to check if access to a given URL is prohibited.  The
same WWW::RobotRules object can be used for one or more parsed
<em>/robots.txt</em> files on any number of hosts.</p>
<p>The following methods are provided:</p>
<dl>
<dt><strong><a name="item_new">$rules = WWW::RobotRules-&gt;<code>new($robot_name)</code></a></strong><br />
</dt>
<dd>
This is the constructor for WWW::RobotRules objects.  The first
argument given to <a href="#item_new"><code>new()</code></a> is the name of the robot.
</dd>
<p></p>
<dt><strong><a name="item_parse">$rules-&gt;parse($robot_txt_url, $content, $fresh_until)</a></strong><br />
</dt>
<dd>
The <a href="#item_parse"><code>parse()</code></a> method takes as arguments the URL that was used to
retrieve the <em>/robots.txt</em> file, and the contents of the file.
</dd>
<p></p>
<dt><strong><a name="item_allowed">$rules-&gt;<code>allowed($uri)</code></a></strong><br />
</dt>
<dd>
Returns TRUE if this robot is allowed to retrieve this URL.
</dd>
<p></p>
<dt><strong><a name="item_agent">$rules-&gt;<code>agent([$name])</code></a></strong><br />
</dt>
<dd>
Get/set the agent name. NOTE: Changing the agent name will clear the robots.txt
rules and expire times out of the cache.
</dd>
<p></p></dl>
<p>
</p>
<hr />
<h1><a name="robots_txt">ROBOTS.TXT</a></h1>
<p>The format and semantics of the ``/robots.txt'' file are as follows
(this is an edited abstract of
&lt;http://www.robotstxt.org/wc/norobots.html&gt; ):</p>
<p>The file consists of one or more records separated by one or more
blank lines. Each record contains lines of the form</p>
<pre>
  &lt;field-name&gt;: &lt;value&gt;</pre>
<p>The field name is case insensitive.  Text after the '#' character on a
line is ignored during parsing.  This is used for comments.  The
following &lt;field-names&gt; can be used:</p>
<dl>
<dt><strong><a name="item_user_2dagent">User-Agent</a></strong><br />
</dt>
<dd>
The value of this field is the name of the robot the record is
describing access policy for.  If more than one <em>User-Agent</em> field is
present the record describes an identical access policy for more than
one robot. At least one field needs to be present per record.  If the
value is '*', the record describes the default access policy for any
robot that has not not matched any of the other records.
</dd>
<p></p>
<dt><strong><a name="item_disallow">Disallow</a></strong><br />
</dt>
<dd>
The value of this field specifies a partial URL that is not to be
visited. This can be a full path, or a partial path; any URL that
starts with this value will not be retrieved
</dd>
<p></p></dl>
<p>
</p>
<hr />
<h1><a name="robots_txt_examples">ROBOTS.TXT EXAMPLES</a></h1>
<p>The following example ``/robots.txt'' file specifies that no robots
should visit any URL starting with ``/cyberworld/map/'' or ``/tmp/'':</p>
<pre>
  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
  Disallow: /tmp/ # these will soon disappear</pre>
<p>This example ``/robots.txt'' file specifies that no robots should visit
any URL starting with ``/cyberworld/map/'', except the robot called
``cybermapper'':</p>
<pre>
  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space</pre>
<pre>
  # Cybermapper knows where to go.
  User-agent: cybermapper
  Disallow:</pre>
<p>This example indicates that no robots should visit this site further:</p>
<pre>
  # go away
  User-agent: *
  Disallow: /</pre>
<p>
</p>
<hr />
<h1><a name="see_also">SEE ALSO</a></h1>
<p><a href="../../../site/lib/LWP/RobotUA.html">the LWP::RobotUA manpage</a>, <a href="../../../site/lib/WWW/RobotRules/AnyDBM_File.html">the WWW::RobotRules::AnyDBM_File manpage</a>
</p>
<table border="0" width="100%" cellspacing="0" cellpadding="3">
<tr><td class="block" valign="middle">
<big><strong><span class="block">&nbsp;WWW::RobotRules - database of robots.txt-derived permissions</span></strong></big>
</td></tr>
</table>

</body>

</html>
